{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":95994,"databundleVersionId":11396223,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# <font style=\"color:blue\">Project 1 - Part 1: Data Understanding & Pipeline Check</font>\n\nHere, we will slightly modify the steps we had used to train Neural Networks:\n\n- Step 1 - Understand your problem\n- Step 2A - Get the data\n- Step 2B - Explore & Understand your data\n- Step 2C - Create a sample data from the dataset\n- Step 3 - Data Preparation\n- Step 4 - Train a simple model on sample data, and check the pipeline before training the full network\n- Step 5 - Train on full Data\n- Step 6 - Improve your model\n\nIn this notebook we will detail the steps 1 to 4, and do some coding along the way! You will implement Steps 5 & 6 in the next notebook.\n\nThis notebook contains 30 points. <font style=\"color:red\">The sections in red are the ones that carry marks.</font>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:20.792780Z","iopub.execute_input":"2025-04-22T03:41:20.793064Z","iopub.status.idle":"2025-04-22T03:41:20.799275Z","shell.execute_reply.started":"2025-04-22T03:41:20.793042Z","shell.execute_reply":"2025-04-22T03:41:20.798255Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_85/4100504795.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    - Step 2A - Get the data\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"],"ename":"SyntaxError","evalue":"invalid decimal literal (4100504795.py, line 6)","output_type":"error"}],"execution_count":13},{"cell_type":"markdown","source":"#### Points Distribution - Maximum Points: 30\n\n\n<div align=\"center\">\n    <table>\n        <tr><td><h3>Number</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Explore the Data</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Data Preparation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Configurations</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Display Mistakes</h3></td> <td><h3>15</h3></td> </tr>\n    </table>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 1: Understand Your Problem </font><a name=\"step1\"></a>\n\nAs you already know, Image Classification helps classify an image based on its visual content. So, the model is supposed to look at the given image and predict which object is present in it. Obviously, the number of objects which it can predict depends on how many you trained it on.\n\nIn our problem, we want to classify an input image between **3 animals** - **_cat, dog and panda_**.\n\n### <font style=\"color:green\">What Do We Need and How to Achieve It? </font>\n\n1. You need correctly-labeled images of each animal.\n2. Also, you need to train a network to understand the input image.\n","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:blue\">Step 2A: Get the Data </font>\n\nLet’s use a dataset from kaggle. <a target=\"_blank\" href=\"https://www.kaggle.com/competitions/open-cv-py-torch-project-1-classification-round-2/data\">Go to the Data Tab</a>.\n\nWe have already separated the dataset into training, validation and test splits for you.","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\n\n\n# Text formatting\nbold = \"\\033[1m\"\nend = \"\\033[0m\"\n\nplt.style.use('ggplot')\nblock_plot=False\n\n%matplotlib inline\n#print(\"Raj\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:20.799634Z","iopub.status.idle":"2025-04-22T03:41:20.799849Z","shell.execute_reply.started":"2025-04-22T03:41:20.799736Z","shell.execute_reply":"2025-04-22T03:41:20.799746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 2B: Explore the Data [5 Points]</font><a name=\"step2b\"></a>\n","metadata":{}},{"cell_type":"code","source":"#root_dir = r\"/kaggle/input/opencv-pytorch-classification-project-1/dataset\"\n\nroot_dir = r\"/kaggle/input/open-cv-py-torch-project-1-classification-round-2/dataset\"\n\ntrain_dir = os.path.join(root_dir, \"Train\")\nvalid_dir = os.path.join(root_dir, \"Valid\")\n\nprint(root_dir)\nprint(train_dir)\nprint(valid_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:20.807886Z","iopub.execute_input":"2025-04-22T03:41:20.808282Z","iopub.status.idle":"2025-04-22T03:41:20.814801Z","shell.execute_reply.started":"2025-04-22T03:41:20.808263Z","shell.execute_reply":"2025-04-22T03:41:20.814035Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/open-cv-py-torch-project-1-classification-round-2/dataset\n/kaggle/input/open-cv-py-torch-project-1-classification-round-2/dataset/Train\n/kaggle/input/open-cv-py-torch-project-1-classification-round-2/dataset/Valid\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### <font style=\"color:green\">Folder Structure </font>\n","metadata":{}},{"cell_type":"code","source":"def list_folders(startpath):\n    for root, _, files in os.walk(startpath):\n        level = root.replace(startpath, \"\").count(os.sep)\n        indent = \" \" * 4 * (level)\n        print(f\"{indent}{os.path.basename(root):<8}\")\n\n\nlist_folders(root_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:20.815725Z","iopub.execute_input":"2025-04-22T03:41:20.816015Z","iopub.status.idle":"2025-04-22T03:41:25.538086Z","shell.execute_reply.started":"2025-04-22T03:41:20.815997Z","shell.execute_reply":"2025-04-22T03:41:25.537383Z"}},"outputs":[{"name":"stdout","text":"dataset \n    Valid   \n        dog     \n        panda   \n        cat     \n    Test    \n    Train   \n        dog     \n        panda   \n        cat     \n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### <font style=\"color:green\">Import the Datasets Module</font>\n\nLoad the data utilizing the `datasets` module.","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:33.393202Z","iopub.execute_input":"2025-04-22T03:41:33.393549Z","iopub.status.idle":"2025-04-22T03:41:33.410703Z","shell.execute_reply.started":"2025-04-22T03:41:33.393526Z","shell.execute_reply":"2025-04-22T03:41:33.409839Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_85/1994238759.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStochasticDepth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_presets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_register_onnx_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_register_custom_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .boxes import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatched_nms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbox_area\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbox_convert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/_register_onnx_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msymbolic_opset11\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopset11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_exporter_states\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportTypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m from ._internal.onnxruntime import (\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mis_onnxrt_backend_supported\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mOrtBackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_OrtBackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/_internal/onnxruntime.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFakeTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_tensor_prop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFakeTensorProp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperator_support\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOperatorSupport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools_common\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCALLABLE_NODE_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/passes/fake_tensor_prop.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFakeTensorProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0mExecute\u001b[0m \u001b[0man\u001b[0m \u001b[0mFX\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mNode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfake\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mrepresenting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2560\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'fx'"],"ename":"AttributeError","evalue":"module 'torch' has no attribute 'fx'","output_type":"error"}],"execution_count":18},{"cell_type":"markdown","source":"### <font style=\"color:green\">Create a Dataset Object </font>\n","metadata":{}},{"cell_type":"code","source":"train_data = datasets.ImageFolder(train_dir)\nvalidation_data = datasets.ImageFolder(valid_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.859290Z","iopub.status.idle":"2025-04-22T03:41:25.859486Z","shell.execute_reply.started":"2025-04-22T03:41:25.859389Z","shell.execute_reply":"2025-04-22T03:41:25.859398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:green\">List the Classes</font>\n\nIt simply prints the sub folders present under the training or validation folder.\n","metadata":{}},{"cell_type":"code","source":"print(train_data.classes)\nprint(validation_data.classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.860567Z","iopub.status.idle":"2025-04-22T03:41:25.860902Z","shell.execute_reply.started":"2025-04-22T03:41:25.860766Z","shell.execute_reply":"2025-04-22T03:41:25.860781Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:green\">Get the Class ID to Name Mapping</font>","metadata":{}},{"cell_type":"code","source":"print(train_data.class_to_idx)\nprint(validation_data.class_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.861765Z","iopub.status.idle":"2025-04-22T03:41:25.862042Z","shell.execute_reply.started":"2025-04-22T03:41:25.861912Z","shell.execute_reply":"2025-04-22T03:41:25.861925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:red\">Find the Number of Samples in Training and Validation Folders [2 Points]</font>\n","metadata":{}},{"cell_type":"code","source":"# Enter Code Here\n\n# Number of samples in training and validation datasets\nprint(f\"{bold}Number of training samples:{end}\", len(train_data))\nprint(f\"{bold}Number of validation samples:{end}\", len(validation_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.863303Z","iopub.status.idle":"2025-04-22T03:41:25.863509Z","shell.execute_reply.started":"2025-04-22T03:41:25.863415Z","shell.execute_reply":"2025-04-22T03:41:25.863424Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:red\">Display Some Samples [3 Points]</font>\n\nDisplay one sample from each class. We know that the train dataset contains\n\n- cats from 0 to 699\n- dogs from 700 to 1399\n- pandas from 1400 to 2099\n\nTake one sample from each class and display using matplotlib\n","metadata":{}},{"cell_type":"code","source":"# Enter Code Here\n\nimg, target = train_data[0]\n\nprint(\"image size: {}, target: {}\".format(img.size, target))\n\nplt.imshow(img)\nplt.axis('off')\nplt.title(\"Cat\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.864461Z","iopub.status.idle":"2025-04-22T03:41:25.864762Z","shell.execute_reply.started":"2025-04-22T03:41:25.864576Z","shell.execute_reply":"2025-04-22T03:41:25.864635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enter Code Here\n\nimg, target = train_data[700]\n\nprint(\"image size: {}, target: {}\".format(img.size, target))\n\nplt.imshow(img)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.865470Z","iopub.status.idle":"2025-04-22T03:41:25.865727Z","shell.execute_reply.started":"2025-04-22T03:41:25.865570Z","shell.execute_reply":"2025-04-22T03:41:25.865598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enter Code Here\n\nimg, target = train_data[1400]\n\nprint(\"image size: {}, target: {}\".format(img.size, target))\n\nplt.imshow(img)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.867301Z","iopub.status.idle":"2025-04-22T03:41:25.867623Z","shell.execute_reply.started":"2025-04-22T03:41:25.867453Z","shell.execute_reply":"2025-04-22T03:41:25.867468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note:** The images are all of different size.\n","metadata":{}},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 2C: Create Sample Data</font><a name=\"step2c\"></a>\n\nTake `5%` images from training and validation to create a small sample dataset, which will check our training pipeline.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.868929Z","iopub.status.idle":"2025-04-22T03:41:25.869240Z","shell.execute_reply.started":"2025-04-22T03:41:25.869087Z","shell.execute_reply":"2025-04-22T03:41:25.869100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subset_size = 0.05","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.870314Z","iopub.status.idle":"2025-04-22T03:41:25.870647Z","shell.execute_reply.started":"2025-04-22T03:41:25.870492Z","shell.execute_reply":"2025-04-22T03:41:25.870506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:green\">Create a Subset From the Original Data</font>\n\nInstead of copying the original data, use the `Subset` method in `torch` to create a subset of the data. And use this to train the model.\n\nThis is new. Let’ss see how it is done.","metadata":{}},{"cell_type":"code","source":"train_subset = torch.utils.data.Subset(train_data, np.arange(0, len(train_data), 1.0 / subset_size))\n\nvalidation_subset = torch.utils.data.Subset(validation_data, np.arange(0, len(validation_data), 1.0 / subset_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.872389Z","iopub.status.idle":"2025-04-22T03:41:25.872725Z","shell.execute_reply.started":"2025-04-22T03:41:25.872544Z","shell.execute_reply":"2025-04-22T03:41:25.872561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_subset_loader = torch.utils.data.DataLoader(train_subset, batch_size=8, num_workers=1, shuffle=False)\n\nvalidation_subset_loader = torch.utils.data.DataLoader(validation_subset, batch_size=8, num_workers=1, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.873824Z","iopub.status.idle":"2025-04-22T03:41:25.874136Z","shell.execute_reply.started":"2025-04-22T03:41:25.873975Z","shell.execute_reply":"2025-04-22T03:41:25.873989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train Subset Size: {}\".format(len(train_subset_loader.dataset)))\nprint(\"Validation Subset Size: {}\".format(len(validation_subset_loader.dataset)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.875041Z","iopub.status.idle":"2025-04-22T03:41:25.875331Z","shell.execute_reply.started":"2025-04-22T03:41:25.875186Z","shell.execute_reply":"2025-04-22T03:41:25.875200Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can see that the subset data is only 5% of the original training and validation data.","metadata":{}},{"cell_type":"markdown","source":"We will create a separate function called `subset_data_loader` to create data loaders for subsets.\n","metadata":{}},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 3. Data Preparation [5 Points]</font> <a name=\"step3\"></a>\n\nNow that you have seen how the data is organized, configure the train and valid loaders to feed the training pipeline.\n","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:blue\">3.1. Import Libraries </font>\n","metadata":{}},{"cell_type":"code","source":"import time\nfrom dataclasses import dataclass\nfrom typing import List, Union, Tuple\n\n\nfrom tqdm import tqdm\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchvision import datasets, transforms\n\nfrom torchmetrics import MeanMetric\nfrom torchmetrics.classification import MulticlassAccuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.875906Z","iopub.status.idle":"2025-04-22T03:41:25.876141Z","shell.execute_reply.started":"2025-04-22T03:41:25.876044Z","shell.execute_reply":"2025-04-22T03:41:25.876053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">3.2. Image Transforms</font>\n\nWe need to normalize the data. Some of the standard ways of doing it is to, subtract the mean, make all images equal in size or to rescale the range to `[0., 1.]` etc. Use the following functions to do this:\n","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:green\">3.2.1. Compulsary Preprocessing Transforms</font>\n","metadata":{}},{"cell_type":"code","source":"def image_preprocess_transforms(img_size):\n    preprocess = transforms.Compose(\n        [\n            transforms.Resize(img_size),\n            transforms.ToTensor(),\n        ]\n    )\n\n    return preprocess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.877112Z","iopub.status.idle":"2025-04-22T03:41:25.877359Z","shell.execute_reply.started":"2025-04-22T03:41:25.877252Z","shell.execute_reply":"2025-04-22T03:41:25.877264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:green\">3.2.2. Common Image Transforms</font>\n\nNormalize using mean and std. You can add any other transforms here as per your dataset.\n","metadata":{}},{"cell_type":"code","source":"def image_common_transforms(img_size=(224, 224), mean=(0.4611, 0.4359, 0.3905), std=(0.2193, 0.2150, 0.2109)):\n    preprocess = image_preprocess_transforms(img_size)\n\n    common_transforms = transforms.Compose(\n        [\n            preprocess,\n            transforms.Normalize(mean, std),\n        ]\n    )\n\n    return common_transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.878475Z","iopub.status.idle":"2025-04-22T03:41:25.878747Z","shell.execute_reply.started":"2025-04-22T03:41:25.878628Z","shell.execute_reply":"2025-04-22T03:41:25.878641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:green\">3.2.3. Mean and STD</font>\n\nFunction for Calculating Mean and Variance","metadata":{}},{"cell_type":"code","source":"def get_mean_std(data_root, img_size=(224, 224), num_workers=4):\n    transform = image_preprocess_transforms(img_size=img_size)\n\n    loader = data_loader(data_root, transform)\n\n    batch_mean = torch.zeros(3)\n    batch_mean_sqrd = torch.zeros(3)\n\n    for batch_data, _ in loader:\n        batch_mean += batch_data.mean(dim=(0, 2, 3))  # E[batch_i]\n        batch_mean_sqrd += (batch_data**2).mean(dim=(0, 2, 3))  #  E[batch_i**2]\n\n    # E[dataset] = E[E[batch_1], E[batch_2], ...]\n    mean = batch_mean / len(loader)\n\n    # var[X] = E[X**2] - E[X]**2\n\n    # E[X**2] = E[E[batch_1**2], E[batch_2**2], ...]\n    # E[X]**2 = E[E[batch_1], E[batch_2], ...] ** 2\n\n    var = (batch_mean_sqrd / len(loader)) - (mean**2)\n\n    std = var**0.5\n    print(\"mean: {}, std: {}\".format(mean, std))\n\n    return mean, std","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.879705Z","iopub.status.idle":"2025-04-22T03:41:25.879913Z","shell.execute_reply.started":"2025-04-22T03:41:25.879817Z","shell.execute_reply":"2025-04-22T03:41:25.879826Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">3.3. Data Loaders </font>\n","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:green\">3.3.1. Data Loader for Full Data</font>\n\nData loader used by the training routine to generate batches of data:\n","metadata":{}},{"cell_type":"code","source":"def data_loader(data_root, transform, batch_size=16, shuffle=False, num_workers=2):\n    dataset = datasets.ImageFolder(root=data_root, transform=transform)\n\n    loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n    )\n\n    return loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.880824Z","iopub.status.idle":"2025-04-22T03:41:25.881163Z","shell.execute_reply.started":"2025-04-22T03:41:25.881006Z","shell.execute_reply":"2025-04-22T03:41:25.881022Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:green\">3.3.2. Data Loader for Subset</font>\n\nData loader which uses the `Subset` to generate batches\n","metadata":{}},{"cell_type":"code","source":"def subset_data_loader(data_root, transform, batch_size=8, shuffle=False, num_workers=2, subset_size=0.05):\n    dataset = datasets.ImageFolder(root=data_root, transform=transform)\n\n    data_subset = torch.utils.data.Subset(\n        dataset,\n        np.arange(0, len(dataset), 1.0 / subset_size).astype(int),\n    )\n\n    loader = torch.utils.data.DataLoader(data_subset, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n\n    return loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.881928Z","iopub.status.idle":"2025-04-22T03:41:25.882217Z","shell.execute_reply.started":"2025-04-22T03:41:25.882069Z","shell.execute_reply":"2025-04-22T03:41:25.882083Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:red\">3.4. Prepare Data [5 Points]</font>\n\nThe main function which uses all the above functions to generate the train and valid dataloaders.\n","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:red\">Question </font>[5 Points]\n\nThere are two mistakes in the below code snippet. Spot the errors and correct them all. (Note: The rest of the code will run, even if you fail to find the mistakes.)\n","metadata":{}},{"cell_type":"code","source":"def get_data(batch_size, data_root, img_size=(224, 224), num_workers=4, data_augmentation=False):\n    train_data_path = os.path.join(data_root, \"Train\")\n\n    mean, std = get_mean_std(data_root=train_data_path, img_size=img_size, num_workers=num_workers)\n\n    common_transforms = image_common_transforms(img_size, mean, std)\n\n    # If data_augmentation is true data augmentation will be applied.\n    if data_augmentation:\n        train_transforms = data_augmentation_preprocess(mean, std)\n    # Else simply do common transforms\n    else:\n        train_transforms = common_transforms\n\n    # Train dataloader\n    train_loader = subset_data_loader(\n        train_data_path,\n        train_transforms,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n    )\n\n    # Valid dataloader\n    valid_data_path = os.path.join(data_root, \"Valid\")\n\n    '''\n    valid_loader = subset_data_loader(\n        valid_data_path,\n        train_transforms,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n    )\n\n    '''\n    valid_loader = data_loader(\n        valid_data_path,\n        common_transforms,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n    )\n\n    return train_loader, valid_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.883333Z","iopub.status.idle":"2025-04-22T03:41:25.883551Z","shell.execute_reply.started":"2025-04-22T03:41:25.883441Z","shell.execute_reply":"2025-04-22T03:41:25.883449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 4: Train a Simple Model</font><a name=\"step4\"></a>\n\nLet's dive into creating the training pipeline and training a simple model on our sample data. Don't worry, we've got most of the code covered for you in this section! In the next notebook, it'll be your turn to take the reins and make all the necessary tweaks and changes. It's going to be an exciting learning journey!","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:red\">4.1. Configurations [ 5 Points]</font>\n\nIn this section, define the training and system configurations. \n\n### <font style=\"color:red\">Question </font>[5 Points]:\n\nSet up the training pipeline with a batch size of `4`. Run the experiment then for `100` epochs. Change the configurations as given below:\n","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:green\">4.1.1. System Configuration</font>\n","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass SystemConfig:\n    \"\"\"\n    Describes the common system setting needed for reproducible training\n    \"\"\"\n\n    seed: int = 21  # Seed number to set the state of all random number generators\n    cudnn_benchmark_enabled: bool = True  # Enable CuDNN benchmark for the sake of performance\n    cudnn_deterministic: bool = True  # Make cudnn deterministic (reproducible training)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.884340Z","iopub.status.idle":"2025-04-22T03:41:25.884526Z","shell.execute_reply.started":"2025-04-22T03:41:25.884437Z","shell.execute_reply":"2025-04-22T03:41:25.884445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def system_setup(config: SystemConfig):\n    random.seed(config.seed)\n    np.random.seed(config.seed)\n    torch.manual_seed(config.seed)\n    torch.cuda.manual_seed_all(config.seed)\n\n    torch.backends.cudnn.benchmark = config.cudnn_benchmark_enabled\n    torch.backends.cudnn.deterministic = config.cudnn_deterministic\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    return device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.885504Z","iopub.status.idle":"2025-04-22T03:41:25.885840Z","shell.execute_reply.started":"2025-04-22T03:41:25.885668Z","shell.execute_reply":"2025-04-22T03:41:25.885682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:green\">4.1.2. Training Configuration</font>\n","metadata":{}},{"cell_type":"code","source":"config = TrainingConfig()  # ✅ Create the config object\n\nmodel = MyModel(num_classes=config.num_classes, dropout_prob=config.dropout_prob).to(config.device)\n\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=config.init_learning_rate,\n    weight_decay=config.weight_decay\n)\n\nscheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=config.scheduler_step_size,\n    gamma=config.scheduler_gamma\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.886715Z","iopub.status.idle":"2025-04-22T03:41:25.887020Z","shell.execute_reply.started":"2025-04-22T03:41:25.886860Z","shell.execute_reply":"2025-04-22T03:41:25.886874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Tuple\nimport os\nimport torch\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"\n    Configuration for the training process\n    \"\"\"\n\n    # Core training settings\n    num_classes: int = 3\n    batch_size: int = 8                      # 🔼 Slight increase for better batch statistics\n    img_size: Tuple[int, int] = (224, 224)\n    epochs_count: int = 50                   # 🔽 Initially lower to allow faster experimentation\n    init_learning_rate: float = 0.001\n    weight_decay: float = 1e-4               # 🛡️ Helps regularization\n    dropout_prob: float = 0.5                # 💧 To use in model\n\n    # Dataset path and loader config\n    data_root: str = \"/kaggle/input/open-cv-py-torch-project-1-classification-round-2/dataset\"\n    num_workers: int = 2\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Normalization stats (used in transforms)\n    mean: Tuple[float, float, float] = (0.4573, 0.4348, 0.3884)\n    std: Tuple[float, float, float] = (0.2686, 0.2601, 0.2600)\n\n    # Logging & checkpointing\n    save_model_name: str = \"cat_dog_panda_classifier.pt\"\n    root_log_dir: str = os.path.join(\"Logs_Checkpoints\", \"Model_logs\")\n    root_checkpoint_dir: str = os.path.join(\"Logs_Checkpoints\", \"Model_checkpoints\")\n    log_dir: str = \"version_0\"\n    checkpoint_dir: str = \"version_0\"\n\n    # Learning rate scheduler\n    scheduler_step_size: int = 15            # 🔁 Decay step\n    scheduler_gamma: float = 0.5             # 📉 Reduce LR by this factor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.888004Z","iopub.status.idle":"2025-04-22T03:41:25.888286Z","shell.execute_reply.started":"2025-04-22T03:41:25.888126Z","shell.execute_reply":"2025-04-22T03:41:25.888139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_augmentation_preprocess(mean, std, img_size=(224, 224)):\n    return transforms.Compose([\n        transforms.Resize(img_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1))\n\n    ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.889148Z","iopub.status.idle":"2025-04-22T03:41:25.889371Z","shell.execute_reply.started":"2025-04-22T03:41:25.889267Z","shell.execute_reply":"2025-04-22T03:41:25.889276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_resnet18_model(num_classes):\n    model = models.resnet18(weights=None)  # <-- offline mode\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.890740Z","iopub.status.idle":"2025-04-22T03:41:25.891048Z","shell.execute_reply.started":"2025-04-22T03:41:25.890896Z","shell.execute_reply":"2025-04-22T03:41:25.890909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_config = TrainingConfig()\n\nfrom torchvision import models\n\ndef get_resnet18_model(num_classes):\n    model = models.resnet18(weights=None)  # <-- Offline-safe\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\nmodel = get_resnet18_model(training_config.num_classes).to(training_config.device)\n\n#model = get_resnet18_model(config.num_classes).to(config.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.891632Z","iopub.status.idle":"2025-04-22T03:41:25.891937Z","shell.execute_reply.started":"2025-04-22T03:41:25.891787Z","shell.execute_reply":"2025-04-22T03:41:25.891800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:green\">4.1.3. System Setup</font>\n","metadata":{}},{"cell_type":"code","source":"def setup_system(system_config: SystemConfig) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(system_config.seed)\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.893294Z","iopub.status.idle":"2025-04-22T03:41:25.893606Z","shell.execute_reply.started":"2025-04-22T03:41:25.893439Z","shell.execute_reply":"2025-04-22T03:41:25.893452Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchmetrics.classification import MulticlassAccuracy\nfrom torchmetrics import MeanMetric\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.895019Z","iopub.status.idle":"2025-04-22T03:41:25.895324Z","shell.execute_reply.started":"2025-04-22T03:41:25.895167Z","shell.execute_reply":"2025-04-22T03:41:25.895182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(\n    train_config: TrainingConfig,\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    train_loader: torch.utils.data.DataLoader,\n    epoch_idx: int,\n    total_epochs: int,\n) -> Tuple[float, float]:\n    \n    # Change model in training mode.\n    model.train()\n\n    acc_metric = MulticlassAccuracy(num_classes=train_config.num_classes, average=\"micro\")\n    mean_metric = MeanMetric()\n\n    device = train_config.device\n\n    status = f\"Train:\\t{bold}Epoch: {epoch_idx}/{total_epochs}{end}\"\n\n    prog_bar = tqdm(train_loader, bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\")\n\n    prog_bar.set_description(status)\n\n    for data, target in prog_bar:\n        # Send data and target to appropriate device.\n        data, target = data.to(device), target.to(device)\n\n        # Reset parameters gradient to zero.\n        optimizer.zero_grad()\n\n        # Forward pass to the model.\n        output = model(data)\n\n        # Cross Entropy loss\n        loss = F.cross_entropy(output, target)\n\n        # Find gradients w.r.t training parameters.\n        loss.backward()\n\n        # Update parameters using gradients.\n        optimizer.step()\n\n        # Batch Loss.\n        mean_metric(loss.item(), weight=data.shape[0])\n\n        # # Get probability score using softmax.\n        # prob = F.softmax(output, dim=1)\n\n        # Get the index of the max probability.\n        pred_idx = output.detach().argmax(dim=1)\n\n        # Batch accuracy.\n        acc_metric(pred_idx.cpu(), target.cpu())\n\n        # Update progress bar description.\n        step_status = status + f\" Train Loss: {mean_metric.compute():.4f}, Train Acc: {acc_metric.compute():.4f}\"\n        prog_bar.set_description(step_status)\n\n    epoch_loss = mean_metric.compute()\n    epoch_acc = acc_metric.compute()\n\n    prog_bar.close()\n\n    return epoch_loss, epoch_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.896291Z","iopub.status.idle":"2025-04-22T03:41:25.896602Z","shell.execute_reply.started":"2025-04-22T03:41:25.896436Z","shell.execute_reply":"2025-04-22T03:41:25.896449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.3. Validation Function</font>\n\nIn the upcoming code cell, we will create the validation function. This function is essential for assessing the performance of our model on unseen data, ensuring its effectiveness and accuracy.\n","metadata":{}},{"cell_type":"code","source":"def validate(\n    train_config: TrainingConfig, \n    model: nn.Module, \n    valid_loader: torch.utils.data.DataLoader,\n    epoch_idx: int, \n    total_epochs: int\n) -> Tuple[float, float]:\n\n    # Change model in evaluation mode.\n    model.eval()\n\n    acc_metric = MulticlassAccuracy(num_classes=train_config.num_classes, average=\"micro\")\n    mean_metric = MeanMetric()\n\n    device = train_config.device\n\n    status = f\"Valid:\\t{bold}Epoch: {epoch_idx}/{total_epochs}{end}\"\n\n    prog_bar = tqdm(valid_loader, bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\")\n\n    prog_bar.set_description(status)\n\n    for data, target in prog_bar:\n        # Send data and target to appropriate device.\n        data, target = data.to(device), target.to(device)\n\n        # Get the model's predicted logits.\n        with torch.no_grad():\n            output = model(data)\n\n        # Compute the CE-Loss.\n        valid_loss = F.cross_entropy(output, target).item()\n\n        # Batch validation loss.\n        mean_metric(valid_loss, weight=data.shape[0])\n\n        # # Convert model's logits to probability scores.\n        # prob = F.softmax(output, dim=1)\n\n        # Get the index of the max probability.\n        pred_idx = output.detach().argmax(dim=1)\n\n        # Batch accuracy.\n        acc_metric(pred_idx.cpu(), target.cpu())\n\n        # Update progress bar description.\n        step_status = status + f\" Valid Loss: {mean_metric.compute():.4f}, Valid Acc: {acc_metric.compute():.4f}\"\n        prog_bar.set_description(step_status)\n\n    valid_loss = mean_metric.compute()\n    valid_acc = acc_metric.compute()\n\n    prog_bar.close()\n\n    return valid_loss, valid_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.897347Z","iopub.status.idle":"2025-04-22T03:41:25.897676Z","shell.execute_reply.started":"2025-04-22T03:41:25.897490Z","shell.execute_reply":"2025-04-22T03:41:25.897503Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.4. Save & Load Model</font>\n\nThe following two code cells are dedicated to essential functions in deep learning model management:\n\n1. **Saving the Model Function**: This function is crucial for preserving the trained model state, allowing us to store the learned parameters for future use or further analysis.\n\n2. **Loading the Model Function**: This function is designed to retrieve and load a previously saved model. It's vital for resuming training, making predictions, or conducting evaluations without having to retrain the model from scratch.\n","metadata":{}},{"cell_type":"code","source":"def save_model(model, device, model_dir=\"models\", model_file_name=\"cat_dog_panda_classifier.pt\"):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # Make sure you transfer the model to cpu.\n    if device == \"cuda\":\n        model.to(\"cpu\")\n\n    # Save the 'state_dict'\n    torch.save(model.state_dict(), model_path)\n\n    if device == \"cuda\":\n        model.to(\"cuda\")\n\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.899140Z","iopub.status.idle":"2025-04-22T03:41:25.899432Z","shell.execute_reply.started":"2025-04-22T03:41:25.899285Z","shell.execute_reply":"2025-04-22T03:41:25.899299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model(model, model_dir=\"models\", model_file_name=\"cat_dog_panda_classifier.pt\", device=torch.device(\"cpu\")):\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # Load model parameters by using 'load_state_dict'.\n    model.load_state_dict(torch.load(model_path, map_location=device))\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.900135Z","iopub.status.idle":"2025-04-22T03:41:25.900418Z","shell.execute_reply.started":"2025-04-22T03:41:25.900270Z","shell.execute_reply":"2025-04-22T03:41:25.900282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.5. Logging Setup</font>\n\nThis function will be initializing directories so that they save tensorboard and model checkpoints for different training versions.\n","metadata":{}},{"cell_type":"code","source":"def setup_log_directory(training_config=TrainingConfig()):\n    \"\"\"Tensorboard Log and Model checkpoint directory Setup\"\"\"\n\n    if os.path.isdir(training_config.root_log_dir):\n        # Get all folders numbers in the root_log_dir.\n        folder_numbers = [int(folder.replace(\"version_\", \"\")) for folder in os.listdir(training_config.root_log_dir)]\n\n        # Find the latest version number present in the log_dir\n        last_version_number = max(folder_numbers)\n\n        # New version name\n        version_name = f\"version_{last_version_number + 1}\"\n\n    else:\n        version_name = training_config.log_dir\n\n    # Update the training config default directory.\n    training_config.log_dir = os.path.join(training_config.root_log_dir, version_name)\n    training_config.checkpoint_dir = os.path.join(training_config.root_checkpoint_dir, version_name)\n\n    # Create new directory for saving new experiment version.\n    os.makedirs(training_config.log_dir, exist_ok=True)\n    os.makedirs(training_config.checkpoint_dir, exist_ok=True)\n\n    print(f\"Logging at: {training_config.log_dir}\")\n    print(f\"Model Checkpoint at: {training_config.checkpoint_dir}\")\n\n    return training_config, version_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.901608Z","iopub.status.idle":"2025-04-22T03:41:25.901899Z","shell.execute_reply.started":"2025-04-22T03:41:25.901749Z","shell.execute_reply":"2025-04-22T03:41:25.901763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.6. Plot Loss and Accuracy</font>\n\nThe next code cell will focus on developing a function for plotting loss and accuracy graphs. This function is instrumental in visualizing the performance of the deep learning model throughout the training process, providing insights into its learning behavior by displaying trends in loss reduction and accuracy improvement over epochs.\n","metadata":{}},{"cell_type":"code","source":"def plot_loss_accuracy(\n    train_loss,\n    val_loss,\n    train_acc,\n    val_acc,\n    colors,\n    loss_legend_loc=\"upper center\",\n    acc_legend_loc=\"upper left\",\n    fig_size=(20, 10),\n    sub_plot1=(1, 2, 1),\n    sub_plot2=(1, 2, 2),\n):\n    plt.rcParams[\"figure.figsize\"] = fig_size\n    fig = plt.figure()\n    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n\n    for i in range(len(train_loss)):\n        x_train = range(len(train_loss[i]))\n        x_val = range(len(val_loss[i]))\n\n        min_train_loss = min(train_loss[i])\n        min_val_loss = min(val_loss[i])\n\n        plt.plot(x_train, train_loss[i], linestyle=\"-\", color=f\"tab:{colors[i]}\", label=f\"TRAIN LOSS ({min_train_loss:.4})\")\n        plt.plot(x_val, val_loss[i], linestyle=\"--\", color=f\"tab:{colors[i]}\", label=f\"VALID LOSS ({min_val_loss:.4})\")\n\n\n    plt.xlabel(\"epoch no.\")\n    plt.ylabel(\"loss\")\n    plt.legend(loc=loss_legend_loc)\n    plt.title(\"Training and Validation Loss\")\n    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n\n    for i in range(len(train_acc)):\n        x_train = range(len(train_acc[i]))\n        x_val = range(len(val_acc[i]))\n\n        max_train_acc = max(train_acc[i])\n        max_val_acc = max(val_acc[i])\n\n        plt.plot(\n            x_train,\n            train_acc[i],\n            linestyle=\"-\",\n            color=f\"tab:{colors[i]}\",\n            label=f\"TRAIN ACC ({max_train_acc:.4})\",\n        )\n\n        plt.plot(\n            x_val,\n            val_acc[i],\n            linestyle=\"--\",\n            color=f\"tab:{colors[i]}\",\n            label=f\"VALID ACC ({max_val_acc:.4})\",\n        )\n\n\n    plt.xlabel(\"epoch no.\")\n    plt.ylabel(\"accuracy\")\n    plt.legend(loc=acc_legend_loc)\n    plt.title(\"Training and Validation Accuracy\")\n    fig.savefig(\"sample_loss_acc_plot.png\")\n    plt.show()\n\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.902833Z","iopub.status.idle":"2025-04-22T03:41:25.903138Z","shell.execute_reply.started":"2025-04-22T03:41:25.902980Z","shell.execute_reply":"2025-04-22T03:41:25.902994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.7. Main Function for Training</font>\n\nIn this function, we integrate all the various functions we've previously defined, creating a cohesive and streamlined workflow.","metadata":{}},{"cell_type":"code","source":"def main(model, summary_writer, scheduler=None, system_config=SystemConfig(), training_config=TrainingConfig(), data_augmentation=True):\n    \n    # Setup system configuration.\n    setup_system(system_config)\n\n    # Initialize data loader\n    train_loader, valid_loader = get_data(\n        batch_size=training_config.batch_size,\n        data_root=training_config.data_root,\n        img_size=training_config.img_size,\n        num_workers=training_config.num_workers,\n        data_augmentation=data_augmentation,\n    )\n\n    # Number of epochs to train.\n    NUM_EPOCHS = training_config.epochs_count\n\n    # Set acceleration device.\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n    # Send model to device (GPU/CPU)\n    model.to(device)\n\n    # Initialize Adam optimizer.\n    optimizer = optim.Adam(model.parameters(), lr=training_config.init_learning_rate)\n\n    best_loss = torch.tensor(np.inf)\n\n    # Epoch train & valid loss accumulator.\n    epoch_train_loss = []\n    epoch_valid_loss = []\n\n    # Epoch train & valid accuracy accumulator.\n    epoch_train_acc = []\n    epoch_valid_acc = []\n\n    # Trainig time measurement\n    t_begin = time.time()\n\n    for epoch in range(NUM_EPOCHS):\n        train_loss, train_acc = train(training_config, model, optimizer, train_loader, epoch + 1, NUM_EPOCHS)\n        val_loss, val_accuracy = validate(training_config, model, valid_loader, epoch + 1, NUM_EPOCHS)\n\n        epoch_train_loss.append(train_loss)\n        epoch_train_acc.append(train_acc)\n\n        epoch_valid_loss.append(val_loss)\n        epoch_valid_acc.append(val_accuracy)\n\n        summary_writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n        summary_writer.add_scalar(\"Accuracy/Train\", train_acc, epoch)\n\n        summary_writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n        summary_writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n\n        if val_loss < best_loss:\n            best_loss = val_loss\n            print(f\"\\nModel Improved... Saving Model ... \", end=\"\")\n            torch.save(model.state_dict(), os.path.join(training_config.checkpoint_dir, training_config.save_model_name))\n            print(\"Done.\\n\")\n\n        print(f\"{'='*72}\\n\")\n\n    print(f\"Total time: {(time.time() - t_begin):.2f}s, Best Loss: {best_loss:.3f}\")\n\n    return epoch_train_loss, epoch_train_acc, epoch_valid_loss, epoch_valid_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.904711Z","iopub.status.idle":"2025-04-22T03:41:25.905062Z","shell.execute_reply.started":"2025-04-22T03:41:25.904916Z","shell.execute_reply":"2025-04-22T03:41:25.904931Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.8. Define Model</font>\n\nIn this section, we will define the architecture of the Convolutional Neural Network (CNN) model and proceed to train it, setting the stage for learning and adapting to our specific dataset through rigorous training cycles.\n","metadata":{}},{"cell_type":"code","source":"'''\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Convolution layers\n        self._body = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),\n        )\n\n        # Fully connected layers\n        self._head = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=64 * 52 * 52, out_features=100),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=100, out_features=3),\n        )\n\n    def forward(self, x):\n        # Apply feature extractor\n        x = self._body(x)\n\n        # Apply classification head\n        x = self._head(x)\n\n        return x\n'''\n\nclass MyModel(nn.Module):\n    def __init__(self, num_classes=3, dropout_prob=0.5):\n        super().__init__()\n\n        self._body = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # smaller kernel, preserve spatial\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n\n        )\n\n        # Dummy input to calculate flattened size\n        self._flattened_size = self._get_flattened_size()\n\n        self._head = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_prob),\n            nn.Linear(512, num_classes),\n        )\n\n        def _get_flattened_size(self):\n            with torch.no_grad():\n                dummy = torch.zeros(1, 3, 224, 224)\n                out = self._body(dummy)\n                return out.view(1, -1).shape[1]\n    \n        def forward(self, x):\n            x = self._body(x)\n            x = self._head(x)\n            return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.905654Z","iopub.status.idle":"2025-04-22T03:41:25.905919Z","shell.execute_reply.started":"2025-04-22T03:41:25.905810Z","shell.execute_reply":"2025-04-22T03:41:25.905822Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.9. Training</font>\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchmetrics.classification import MulticlassAccuracy\nfrom torchmetrics import MeanMetric\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.906994Z","iopub.status.idle":"2025-04-22T03:41:25.907267Z","shell.execute_reply.started":"2025-04-22T03:41:25.907121Z","shell.execute_reply":"2025-04-22T03:41:25.907135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MyModel()\nprint(model)\n\ntraining_config = TrainingConfig()\n\n# Model checkpoint log dir setup.\ntraining_config, current_version_name = setup_log_directory(training_config)\n\n# Tensorboard log dir setup.\nsummary_writer = SummaryWriter(training_config.log_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.908335Z","iopub.status.idle":"2025-04-22T03:41:25.908646Z","shell.execute_reply.started":"2025-04-22T03:41:25.908484Z","shell.execute_reply":"2025-04-22T03:41:25.908497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms, datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.909869Z","iopub.status.idle":"2025-04-22T03:41:25.910186Z","shell.execute_reply.started":"2025-04-22T03:41:25.910020Z","shell.execute_reply":"2025-04-22T03:41:25.910034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and Validate\ntrain_loss, train_acc, val_loss, val_acc = main(\n    model,\n    summary_writer=summary_writer,\n    scheduler=None,\n    system_config=SystemConfig(),\n    training_config=training_config,\n    data_augmentation=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.910815Z","iopub.status.idle":"2025-04-22T03:41:25.911072Z","shell.execute_reply.started":"2025-04-22T03:41:25.910957Z","shell.execute_reply":"2025-04-22T03:41:25.910970Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.10. Loss and Accuracy Plot</font>\n","metadata":{}},{"cell_type":"code","source":"plot_loss_accuracy(\n    train_loss=[train_loss],\n    val_loss=[val_loss],\n    train_acc=[train_acc],\n    val_acc=[val_acc],\n    colors=[\"blue\"],\n    loss_legend_loc=\"upper center\",\n    acc_legend_loc=\"upper left\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.912337Z","iopub.status.idle":"2025-04-22T03:41:25.912646Z","shell.execute_reply.started":"2025-04-22T03:41:25.912480Z","shell.execute_reply":"2025-04-22T03:41:25.912493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 5: Sample Prediction</font><a name=\"predictions\"></a>\n\nShow some sample predictions.\n","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:blue\">5.1. Make Predictions</font>\n","metadata":{}},{"cell_type":"code","source":"def prediction(model, device, batch_input):\n    data = batch_input.to(device)\n\n    with torch.no_grad():\n        output = model(data)\n\n    # Score to probability using softmax.\n    prob = F.softmax(output, dim=1)\n\n    # Get the max probability.\n    pred_prob = prob.data.max(dim=1)[0]\n\n    # Get the index of the max probability.\n    pred_index = prob.data.max(dim=1)[1]\n\n    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.913489Z","iopub.status.idle":"2025-04-22T03:41:25.913803Z","shell.execute_reply.started":"2025-04-22T03:41:25.913651Z","shell.execute_reply":"2025-04-22T03:41:25.913666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">5.2. Get Predictions on a Batch</font>\n","metadata":{}},{"cell_type":"code","source":"def get_sample_prediction(model, data_root, img_size, mean, std):\n    batch_size = 15\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n        num_workers = 8\n    else:\n        device = \"cpu\"\n        num_workers = 2\n\n    # It is important to do model.eval() before prediction.\n    model.eval()\n\n    # Send model to cpu/cuda according to your system configuration.\n    model.to(device)\n\n    # Transformed data\n    valid_dataset_trans = datasets.ImageFolder(root=data_root, transform=image_common_transforms(img_size, mean, std))\n\n    # Original image dataset\n    valid_dataset = datasets.ImageFolder(root=data_root, transform=image_preprocess_transforms(img_size))\n\n    data_len = valid_dataset.__len__()\n\n    interval = int(data_len / batch_size)\n\n    imgs = []\n    inputs = []\n    targets = []\n    for i in range(batch_size):\n        index = i * interval\n        trans_input, target = valid_dataset_trans.__getitem__(index)\n        img, _ = valid_dataset.__getitem__(index)\n\n        imgs.append(img)\n        inputs.append(trans_input)\n        targets.append(target)\n\n    inputs = torch.stack(inputs)\n\n    cls, prob = prediction(model, device, batch_input=inputs)\n\n    plt.style.use(\"default\")\n    plt.rcParams[\"figure.figsize\"] = (15, 9)\n    fig = plt.figure()\n\n    for i, target in enumerate(targets):\n        plt.subplot(3, 5, i + 1)\n        img = transforms.functional.to_pil_image(imgs[i])\n        plt.imshow(img)\n        plt.gca().set_title(f\"P:{valid_dataset.classes[cls[i]]}({prob[i]:.2}), T:{valid_dataset.classes[targets[i]]}\")\n    plt.show()\n\n    return","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-22T03:41:25.915051Z","iopub.status.idle":"2025-04-22T03:41:25.915250Z","shell.execute_reply.started":"2025-04-22T03:41:25.915157Z","shell.execute_reply":"2025-04-22T03:41:25.915166Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:blue\">5.3. Load Model and Run Inference</font>\n","metadata":{}},{"cell_type":"markdown","source":"Next, we will reload the best saved model and use the `get_sample_prediction` function to make some sample predictions. This step is instrumental in visually assessing the performance of our model on the validation dataset, providing a quick and practical insight into how well our model generalizes to new, unseen data.\n","metadata":{}},{"cell_type":"code","source":"trained_model = MyModel()\ntrained_model = load_model(\n    trained_model, \n    model_dir=training_config.checkpoint_dir, \n    model_file_name=training_config.save_model_name\n)\n\ntrain_data_path = os.path.join(training_config.data_root, \"Train\")\nvalid_data_path = os.path.join(training_config.data_root, \"Valid\")\n\nmean, std = get_mean_std(train_data_path, img_size=training_config.img_size)\n\nget_sample_prediction(trained_model, valid_data_path, img_size=training_config.img_size, mean=mean, std=std)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.915910Z","iopub.status.idle":"2025-04-22T03:41:25.916175Z","shell.execute_reply.started":"2025-04-22T03:41:25.916023Z","shell.execute_reply":"2025-04-22T03:41:25.916038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Despite training the network on a very small subset of data, you can see that the predictions are not that bad. This means that our model is ready for training.\n","metadata":{}},{"cell_type":"markdown","source":"# <font style=\"color:red\">Step 6. Display Mistakes [15 points] </font><a name=\"display\"></a>\n\nThe code shown above demonstrates sample predictions. However, since correct predictions don't highlight areas for improvement, it's essential to focus on the errors. Therefore, let's write a similar function specifically designed to display only the mispredictions made by the network, allowing us to identify and address the shortcomings in our model.\n\n**You have to display only 15 images.**\n","metadata":{}},{"cell_type":"code","source":"train_loader, valid_loader = get_data(\n    batch_size=training_config.batch_size,\n    data_root=training_config.data_root,\n    img_size=training_config.img_size,\n    num_workers=training_config.num_workers,\n    data_augmentation=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.916891Z","iopub.status.idle":"2025-04-22T03:41:25.917083Z","shell.execute_reply.started":"2025-04-22T03:41:25.916992Z","shell.execute_reply":"2025-04-22T03:41:25.917001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_misclassified_images(\n    model=model,\n    dataloader=valid_loader,\n    class_names=[\"cat\", \"dog\", \"panda\"],\n    device=training_config.device,\n    mean=(0.4573, 0.4348, 0.3884),\n    std=(0.2686, 0.2601, 0.2600),\n    sample_size=15,\n    collect_limit=500  # increase the limit for deeper look\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.918028Z","iopub.status.idle":"2025-04-22T03:41:25.918230Z","shell.execute_reply.started":"2025-04-22T03:41:25.918137Z","shell.execute_reply":"2025-04-22T03:41:25.918145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as TF\nimport random\nfrom collections import Counter\n\ndef denormalize(tensor, mean, std):\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n    return tensor\n\ndef display_misclassified_images(model, dataloader, class_names, device=\"cpu\", mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0), sample_size=15, collect_limit=100):\n    model.eval()\n    model.to(device)\n\n    misclassified = []\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            preds = outputs.argmax(dim=1)\n\n            mismatches = preds != labels\n            if mismatches.any():\n                for img, true, pred in zip(images[mismatches], labels[mismatches], preds[mismatches]):\n                    misclassified.append((img.cpu(), true.cpu(), pred.cpu()))\n                    if len(misclassified) >= collect_limit:\n                        break\n            if len(misclassified) >= collect_limit:\n                break\n\n    if not misclassified:\n        print(\"🎉 No misclassifications found!\")\n        return\n\n    # Optional: Breakdown by true class\n    label_counts = Counter([class_names[true] for _, true, _ in misclassified])\n    print(\"🔍 Misclassification breakdown by true label:\")\n    for label, count in label_counts.items():\n        print(f\"  {label}: {count}\")\n\n    # Randomly sample 15 to display\n    sampled = random.sample(misclassified, k=min(sample_size, len(misclassified)))\n\n    # Plot\n    plt.figure(figsize=(15, 10))\n    for idx, (img, true, pred) in enumerate(sampled):\n        img = denormalize(img.clone(), mean, std)\n        img = torch.clamp(img, 0, 1)\n        img = TF.to_pil_image(img)\n\n        plt.subplot(3, 5, idx + 1)\n        plt.imshow(img)\n        plt.title(f\"True: {class_names[true]}\\nPred: {class_names[pred]}\")\n        plt.axis(\"off\")\n\n    plt.suptitle(\"Random 15 Misclassified Images\", fontsize=18)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.918886Z","iopub.status.idle":"2025-04-22T03:41:25.919185Z","shell.execute_reply.started":"2025-04-22T03:41:25.919027Z","shell.execute_reply":"2025-04-22T03:41:25.919043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torchmetrics.classification import MulticlassAccuracy\nfrom torchmetrics import MeanMetric\n\ndef evaluate_model(model, dataloader, num_classes=3, device=\"cpu\"):\n    model.eval()\n    model.to(device)\n\n    acc_metric = MulticlassAccuracy(num_classes=num_classes, average=\"micro\")\n    loss_metric = MeanMetric()\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n            loss = F.cross_entropy(outputs, labels)\n\n            preds = outputs.argmax(dim=1)\n            acc_metric.update(preds.cpu(), labels.cpu())\n            loss_metric.update(loss.item())  # ✅ FIXED here\n\n    acc = acc_metric.compute()\n    avg_loss = loss_metric.compute()\n\n    print(f\"✅ Model Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n    print(f\"📉 Average Loss: {avg_loss:.4f}\")\n\n    return acc, avg_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.920982Z","iopub.status.idle":"2025-04-22T03:41:25.921230Z","shell.execute_reply.started":"2025-04-22T03:41:25.921131Z","shell.execute_reply":"2025-04-22T03:41:25.921141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torchmetrics.classification import MulticlassAccuracy\nfrom torchmetrics import MeanMetric\n\ndef evaluate_model(model, dataloader, config):\n    \"\"\"\n    Evaluate the model on the given dataloader and print accuracy and loss.\n    \"\"\"\n    model.eval()\n    model.to(config.device)\n\n    accuracy = MulticlassAccuracy(num_classes=config.num_classes, average=\"micro\").to(config.device)\n    mean_loss = MeanMetric().to(config.device)\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(config.device), labels.to(config.device)\n\n            outputs = model(images)\n            loss = F.cross_entropy(outputs, labels)\n            preds = outputs.argmax(dim=1)\n\n            accuracy.update(preds, labels)\n            mean_loss.update(loss)\n\n    acc_value = accuracy.compute().item()\n    loss_value = mean_loss.compute().item()\n\n    print(f\"✅ Model Accuracy: {acc_value:.4f} ({acc_value * 100:.2f}%)\")\n    print(f\"📉 Average Loss: {loss_value:.4f}\")\n\n    return acc_value, loss_value\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.922408Z","iopub.status.idle":"2025-04-22T03:41:25.922705Z","shell.execute_reply.started":"2025-04-22T03:41:25.922565Z","shell.execute_reply":"2025-04-22T03:41:25.922590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After training is done:\nevaluate_model(model, valid_loader, config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:25.923400Z","iopub.status.idle":"2025-04-22T03:41:25.923755Z","shell.execute_reply.started":"2025-04-22T03:41:25.923545Z","shell.execute_reply":"2025-04-22T03:41:25.923559Z"}},"outputs":[],"execution_count":null}]}